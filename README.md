## LLM Experiments
We use `langchain` library to run all examples. It supports many model backends and provides nice framework to work with
large language models. 

We use `llama.cpp` as a backend to run quantized models as we run everything locally on consumer 
grade hardware. Another nice feature of `llama.cpp` is ability to run models on both CPU and GPU or split model layers 
between CPU and GPU, that allow us to run models that will not fit into GPU memory, but still benefit from GPU optimizations.

Another option for backend is `ctransformers`, it supports same features as `llama.cpp`, but also can run non llama 
based models like falcon. For now falcon implementation has experimental status and does not always produce meaningful 
result, so we will not use it.

We will use two smaller models for our experiments. I think it is better to have multiple small specialized models, then 
try to find one ideal 'do-it-all' model, because we will be able to use smaller models. There are leaderboards for models 
like https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard that have multiple benchmarks. So I'd stark with 
choosing or creating benchmark for each use case.

We only use locally running models, no connection to external APIs required.

# 0. Preparation
We use pyhton 3.10

We will use two models, download them to `model` folder:
```
https://huggingface.co/TheBloke/wizardLM-7B-GGML/resolve/main/wizardLM-7B.ggmlv3.q5_1.bin
https://huggingface.co/TheBloke/Llama-2-13B-GGML/resolve/main/llama-2-13b.ggmlv3.q4_1.bin
```

Install dependencies:
```
pip install -r requirements.txt
```

In order to use GPU acceleration with CUDA on windows we need to compile llama.cpp with CUDA enabled:
```
$Env:LLAMA_CUBLAS = "1"; 
$Env:FORCE_CMAKE = "1"; 
$Env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"; 
pip install --upgrade --force-reinstall --no-binary llama-cpp-python llama-cpp-python==0.1.62; 
Remove-Item Env:\LLAMA_CUBLAS; 
Remove-Item Env:\CMAKE_ARGS; 
Remove-Item Env:\FORCE_CMAKE;
```
If GPU acceleration enabled llama.cpp will output this lines when started:
```
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1080, compute capability 6.1
```
```
llama_model_load_internal: offloading 32 repeating layers to GPU
llama_model_load_internal: offloading non-repeating layers to GPU
llama_model_load_internal: offloading v cache to GPU
llama_model_load_internal: offloading k cache to GPU
llama_model_load_internal: offloaded 35/35 layers to GPU
```

In order to use GPU acceleration with OpenCL on windows for AMD gpu we need to compile llama.cpp with CLBlast enabled.

- Get compiled CLBlast library from https://github.com/CNugteren/CLBlast/releases and extract it. `CLBlast_DIR` 
environment variable should point to `lib\cmake\CLBlast` folder in extracted library.
- Get vcpkg package manager, follow installation instruction from https://github.com/microsoft/vcpkg#quick-start-windows
- Install opencl library `vcpkg install opencl:x64-windows`
- Compile `llama-cpp-pythin` with CLBlast support: 
```
$Env:CLBlast_DIR = "D:\Projects\llm_demo\CLBlast-1.6.1-windows-x64\lib\cmake\CLBlast"; 
$Env:FORCE_CMAKE = "1"; 
$Env:CMAKE_ARGS="-DLLAMA_CLBLAST=on"; 
pip install --upgrade --force-reinstall --no-binary llama-cpp-python llama-cpp-python==0.1.77; 
Remove-Item Env:\CLBlast_DIR; 
Remove-Item Env:\FORCE_CMAKE; 
Remove-Item Env:\CMAKE_ARGS;
```
We need to load opencl and CLBlast libraries:
```
import ctypes

ctypes.cdll.LoadLibrary("D:/Projects/llm_demo/CLBlast-1.6.1-windows-x64/lib/clblast.dll")
ctypes.cdll.LoadLibrary("c:/vcpkg/packages/opencl_x64-windows/bin/OpenCL.dll")
```
If there are multiple gpus with opencl support, in our python file we can specify which platform and gpu to use:
```
import os
os.environ["GGML_OPENCL_PLATFORM"] = "AMD"
os.environ["GGML_OPENCL_DEVICE"] = "1"
```
If GPU acceleration with OpenCL enabled llama.cpp will output this lines when started:
```
ggml_opencl: selecting platform: 'AMD Accelerated Parallel Processing'
ggml_opencl: selecting device: 'gfx1030'
```
```
llama_model_load_internal: offloading 32 repeating layers to GPU
llama_model_load_internal: offloading non-repeating layers to GPU
llama_model_load_internal: offloading v cache to GPU
llama_model_load_internal: offloading k cache to GPU
llama_model_load_internal: offloaded 35/35 layers to GPU
```

# 1. Make sure model can run locally
And can produce meaningful result:
```
python 1_model_test.py
```
Answer generated by model looks reasonable:
```
First, let's find out when Justin Bieber was born. According to Wikipedia, he was born on March 1, 1994.
Now, let's find out which NFL team won the Super Bowl that year. The Super Bowl is played in February of each year, so we need to look at the Super Bowls from 1993 and 1994.
The Super Bowl in 1993 was won by the Dallas Cowboys.
The Super Bowl in 1994 was won by the San Francisco 49ers.
So, the correct answer is the San Francisco 49ers.
```

# 2. Let model use tool
We can let model use external tools. In this simple example the tool we provide simply multiply two numbers, but 
providing tools to model is very powerful option as tool is a python function, so everything that can be programmed can 
become tool used by our model.

We ask model to generate valid json so that we can parse it, call python function and return result to model so that it 
can generate final answer.
```
python 2_1_tool.py
```
Text generated by mode:
```
Action: multiplier(5433, 765) -> 381250
```
We can see that it is aware of our tool, but it did dot generate correct json, instead it tried to suggest wrong answer.

This happened because when working with LLMs there is not only usual programming part, but also prompt engineering part
and `langchain` has prompts developed to use it with chatgpt, not with our models. So we need to modify prompts. 
```
python 2_2_tool_custom_prompt.py
```
This time generated output follow instruction - it is correct JSON:
```
I am happy to help! Let's break down the problem and find a solution. 
Thought: We need to find the product of 5433 and 765. 
Action: 
/```
{
"action": "multiplier",
"action_input": {
"a": 5433,
"b": 765
}
}
/```
```
Then our tool returned result for model to observe:
```
Observation: 4156245.0
```
And produce final result:
```
Thought: That's the product we were looking for! 
Action: 
/```
{
"action": "Final Answer",
"action_input": "The product of 5433 and 765 is 4156245.0"
}
/```
```

# 3. Code generation
We can ask model to generate SQL code. We will use sqlite chinook database from here:
```
https://database.guide/2-sample-databases-sqlite/
```
Make sure sqlite db file is located at `./db/sqlite/Chinook.db`
This database represents music shop, it has tables:
```
Album
Artist
Customer
Employee
Genre
Invoice
InvoiceLine
MediaType
Playlist
PlaylistTrack
Track
```
We want to explore data in `Track` table, that has columns:
```
TrackId
Name
AlbumId
MediaTypeId
GenreId
Composer
Milliseconds
Bytes
UnitPrice
```
We ask model to answer next question:
```
How many tracks that have word 'man' in their name and longer than 300000 milliseconds are there?
```
Run:
```
python 3_1_db_chain.py
```
Model tried to generate valid SQL, but failed. It generated answer by itself instead of calling db:
```
How many tracks that have word 'man' in their name and longer than 300000 milliseconds are there?
SQLQuery:SELECT COUNT(*) FROM Track WHERE MEDIA_TYPE_ID IN (SELECT MEDIA_TYPE_ID FROM MediaType WHERE Name = 'mp3') AND LENGTH(Name) > 3 AND LOWER(Name) LIKE '%man%' 
Answer: There are 5 tracks that meet the criteria.
```
Model was confused by all tables description, we can help it by only providing information about `Track` table:
```
python 3_2_db_chain.py
```
Now model generated correct SQL query, observed result and generated correct final answer:
```
How many tracks that have word 'man' in their name and longer than 300000 milliseconds are there?
SQLQuery:SELECT COUNT(*) FROM Track WHERE Name LIKE '%man%' AND Milliseconds > 300000;
SQLResult: [(37,)]
Answer: There are 37 tracks that have word 'man' in their name and longer than 300000 milliseconds.
```
We solved issue, but limiting tables available is not the best solution. Let's try different model - llama 2:
```
python 3_3_db_chain_l2.py
```
Here we passed information about all tables and got correct answer:
```
How many tracks that have word 'man' in their name and longer than 300000 milliseconds are there?
SQLQuery:SELECT COUNT(*) FROM Track WHERE Name LIKE '%man%' AND Milliseconds > 300000;
SQLResult: [(37,)]
Answer: 37
```